#!/usr/bin/env python3
"""Unified Confluence loader - –æ–±'—î–¥–Ω—É—î qa_loader.py —Ç–∞ confluence_loader.py —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å."""

import os
import sys
import hashlib
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import click
import tiktoken
from dataclasses import dataclass

# Add app to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from app.config import settings
from app.data.qa_repository import QARepository
from app.data.vectordb_repo import VectorDBRepository
from app.models.qa_models import QASection, Checklist, TestCase, Config, IngestionJob
from app.ai import OpenAIEmbedder
from app.ai.qa_analyzer import QAContentAnalyzer
try:
    from .confluence_mock import MockConfluenceAPI
    from .confluence_real import RealConfluenceAPI
    from .enhanced_qa_analyzer import EnhancedQAAnalyzer
    from .html_table_parser import EnhancedConfluenceTableParser
except ImportError:
    # Fallback for direct script execution
    from confluence_mock import MockConfluenceAPI
    from confluence_real import RealConfluenceAPI
    from enhanced_qa_analyzer import EnhancedQAAnalyzer
    from html_table_parser import EnhancedConfluenceTableParser


@dataclass
class LoadingProgress:
    """–ö–ª–∞—Å –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è."""
    total_pages: int = 0
    processed_pages: int = 0
    total_checklists: int = 0
    processed_checklists: int = 0
    skipped_checklists: int = 0
    created_checklists: int = 0
    created_testcases: int = 0
    created_configs: int = 0
    sections_processed: int = 0
    chunks_created: int = 0
    
    def get_page_progress_percent(self) -> float:
        if self.total_pages == 0:
            return 0.0
        return (self.processed_pages / self.total_pages) * 100


class ChunkProcessor:
    """Text chunking processor."""
    
    def __init__(self, chunk_size: int = None, chunk_overlap: int = None):
        """Initialize chunker."""
        self.chunk_size = chunk_size or settings.chunk_size
        self.chunk_overlap = chunk_overlap or settings.chunk_overlap
        self.tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4 tokenizer
    
    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        return len(self.tokenizer.encode(text))
    
    def chunk_text(self, text: str) -> List[str]:
        """Split text into overlapping chunks."""
        if not text.strip():
            return []
        
        # Split by paragraphs first
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for paragraph in paragraphs:
            paragraph_tokens = self.count_tokens(paragraph)
            
            # If single paragraph is too big, split it
            if paragraph_tokens > self.chunk_size:
                # Save current chunk if it has content
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                
                # Split large paragraph by sentences
                sentences = [s.strip() + '.' for s in paragraph.split('.') if s.strip()]
                chunk_sentences = []
                chunk_tokens = 0
                
                for sentence in sentences:
                    sentence_tokens = self.count_tokens(sentence)
                    
                    if chunk_tokens + sentence_tokens > self.chunk_size and chunk_sentences:
                        chunks.append(' '.join(chunk_sentences))
                        # Keep some overlap
                        overlap_sentences = chunk_sentences[-2:] if len(chunk_sentences) > 2 else chunk_sentences
                        chunk_sentences = overlap_sentences + [sentence]
                        chunk_tokens = sum(self.count_tokens(s) for s in chunk_sentences)
                    else:
                        chunk_sentences.append(sentence)
                        chunk_tokens += sentence_tokens
                
                if chunk_sentences:
                    chunks.append(' '.join(chunk_sentences))
                
                current_chunk = ""
                current_tokens = 0
                continue
            
            # Check if adding this paragraph would exceed chunk size
            if current_tokens + paragraph_tokens > self.chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                
                # Create overlap with previous chunk
                overlap_text = self._get_overlap_text(current_chunk)
                current_chunk = overlap_text + "\n\n" + paragraph if overlap_text else paragraph
                current_tokens = self.count_tokens(current_chunk)
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
                current_tokens += paragraph_tokens
        
        # Add final chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _get_overlap_text(self, chunk: str) -> str:
        """Get overlap text from the end of a chunk."""
        overlap_tokens = 0
        sentences = chunk.split('.')
        overlap_sentences = []
        
        # Take sentences from the end until we reach overlap size
        for sentence in reversed(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
            
            sentence_tokens = self.count_tokens(sentence)
            if overlap_tokens + sentence_tokens > self.chunk_overlap:
                break
            
            overlap_sentences.insert(0, sentence)
            overlap_tokens += sentence_tokens
        
        return '. '.join(overlap_sentences) + '.' if overlap_sentences else ""


class UnifiedConfluenceLoader:
    """–û–±'—î–¥–Ω–∞–Ω–∏–π –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—á Confluence –¥–∞–Ω–∏—Ö –¥–ª—è MySQL —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏."""
    
    def __init__(self, use_mock: bool = True, load_mysql: bool = True, load_vector: bool = True, use_enhanced_analysis: bool = True):
        """Initialize unified loader."""
        self.use_mock = use_mock
        self.load_mysql = load_mysql
        self.load_vector = load_vector
        self.use_enhanced_analysis = use_enhanced_analysis
        self.progress = LoadingProgress()
        
        # Initialize repositories
        if self.load_mysql:
            self.qa_repo = QARepository()
            self.qa_analyzer = EnhancedQAAnalyzer()  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–π –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä
            self.html_parser = EnhancedConfluenceTableParser()  # –î–æ–¥–∞—î–º–æ HTML –ø–∞—Ä—Å–µ—Ä
            self._existing_checklists = set()
            self._load_existing_checklists()
        
        if self.load_vector:
            self.vector_repo = VectorDBRepository()
            self.embedder = OpenAIEmbedder()
            self.chunker = ChunkProcessor()
        
        # Initialize Confluence API
        if use_mock:
            self.confluence_api = MockConfluenceAPI()
        else:
            self.confluence_api = RealConfluenceAPI()
    
    def _load_existing_checklists(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å–ø–∏—Å–æ–∫ —ñ—Å–Ω—É—é—á–∏—Ö —á–µ–∫–ª—ñ—Å—Ç—ñ–≤ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫—É."""
        try:
            session = self.qa_repo.get_session()
            existing = session.query(Checklist.confluence_page_id).all()
            self._existing_checklists = {page_id[0] for page_id in existing}
            click.echo(f"üìã –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self._existing_checklists)} —ñ—Å–Ω—É—é—á–∏—Ö —á–µ–∫–ª—ñ—Å—Ç—ñ–≤ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫—É")
            session.close()
        except Exception as e:
            click.echo(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —ñ—Å–Ω—É—é—á—ñ —á–µ–∫–ª—ñ—Å—Ç: {e}")
    
    async def load_data(
        self,
        page_ids: Optional[List[str]] = None,
        space_keys: Optional[List[str]] = None,
        labels: Optional[List[str]] = None,
        updated_since: Optional[str] = None,
        limit: Optional[int] = None
    ) -> Dict[str, Any]:
        """–û–±'—î–¥–Ω–∞–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –≤ MySQL —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É."""
        
        click.echo(f"üöÄ –û–±'—î–¥–Ω–∞–Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Confluence –¥–∞–Ω–∏—Ö...")
        click.echo(f"üìä MySQL –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {'‚úÖ' if self.load_mysql else '‚ùå'}")
        click.echo(f"üîç –í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞: {'‚úÖ' if self.load_vector else '‚ùå'}")
        click.echo(f"üåê Confluence API: {'Mock' if self.use_mock else 'Real'}")
        
        if page_ids:
            click.echo(f"üìÑ Page IDs: {', '.join(page_ids)}")
        if space_keys:
            click.echo(f"üè¢ Spaces: {', '.join(space_keys)}")
        if labels:
            click.echo(f"üè∑Ô∏è Labels: {', '.join(labels)}")
        if limit:
            click.echo(f"üìã –õ—ñ–º—ñ—Ç: {limit} —á–µ–∫–ª—ñ—Å—Ç—ñ–≤")
        
        # Parse updated_since
        since_date = None
        if updated_since:
            since_date = datetime.fromisoformat(updated_since)
        
        # Fetch pages
        if page_ids and not self.use_mock:
            # Load specific pages with children for real API
            pages = self.confluence_api.get_pages_by_ids(
                page_ids=page_ids,
                include_children=True
            )
        else:
            # Use regular filtering method
            pages = self.confluence_api.get_pages(
                space_keys=space_keys,
                labels=labels,
                updated_since=since_date
            )
        
        click.echo(f"üìÑ –ó–Ω–∞–π–¥–µ–Ω–æ {len(pages)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è –æ–±—Ä–æ–±–∫–∏")
        self.progress.total_pages = len(pages)
        self.progress.total_checklists = len(pages)
        
        # Create ingestion job
        job_desc = f"Unified loading: {len(pages)} pages, MySQL: {self.load_mysql}, Vector: {self.load_vector}"
        if limit:
            job_desc += f", max {limit} checklists"
        job = self._create_ingestion_job(job_desc) if self.load_mysql else None
        
        try:
            documents_processed = 0
            chunks_created = 0
            
            for i, page in enumerate(pages):
                if limit and self.progress.created_checklists >= limit:
                    click.echo(f"üõë –î–æ—Å—è–≥–Ω—É—Ç–æ –ª—ñ–º—ñ—Ç {limit} —á–µ–∫–ª—ñ—Å—Ç, –∑—É–ø–∏–Ω—è—î–º–æ")
                    break
                
                try:
                    click.echo(f"\nüîÑ –û–±—Ä–æ–±–∫–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {i+1}/{len(pages)}: {page['title']}")
                    
                    # Process page for both MySQL and Vector DB
                    result = await self._process_page_unified(page)
                    
                    documents_processed += 1
                    chunks_created += result.get('chunks_created', 0)
                    
                    if result.get('mysql_success'):
                        self.progress.created_checklists += 1
                        self.progress.created_testcases += result.get('testcases_created', 0)
                        self.progress.created_configs += result.get('configs_created', 0)
                        click.echo(f"  ‚úÖ MySQL: –°—Ç–≤–æ—Ä–µ–Ω–æ {result.get('testcases_created', 0)} —Ç–µ—Å—Ç–∫–µ–π—Å—ñ–≤")
                    else:
                        self.progress.skipped_checklists += 1
                        click.echo(f"  ‚è≠Ô∏è MySQL: –ü—Ä–æ–ø—É—â–µ–Ω–æ ({result.get('mysql_reason', 'Unknown')})")
                    
                    if result.get('vector_success'):
                        click.echo(f"  ‚úÖ Vector: –°—Ç–≤–æ—Ä–µ–Ω–æ {result.get('chunks_created', 0)} —á–∞–Ω–∫—ñ–≤")
                    else:
                        click.echo(f"  ‚è≠Ô∏è Vector: –ü—Ä–æ–ø—É—â–µ–Ω–æ ({result.get('vector_reason', 'Unknown')})")
                    
                    self.progress.processed_pages += 1
                    
                except Exception as e:
                    click.echo(f"  ‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {e}", err=True)
                    continue
            
            # Update job
            if job:
                self._update_ingestion_job(job, "success", {
                    'checklists': self.progress.created_checklists,
                    'testcases': self.progress.created_testcases,
                    'configs': self.progress.created_configs,
                    'chunks': chunks_created,
                    'skipped': self.progress.skipped_checklists
                })
            
            self._print_final_summary()
            
            return {
                "success": True,
                "documents_processed": documents_processed,
                "checklists_created": self.progress.created_checklists,
                "testcases_created": self.progress.created_testcases,
                "configs_created": self.progress.created_configs,
                "chunks_created": chunks_created,
                "skipped_checklists": self.progress.skipped_checklists
            }
            
        except Exception as e:
            if job:
                self._update_ingestion_job(job, "failed", {'error': str(e)})
            click.echo(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            raise
    
    async def _process_page_unified(self, page: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–æ–±–ª—è—î –æ–¥–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É –¥–ª—è MySQL —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏."""
        result = {
            'mysql_success': False,
            'vector_success': False,
            'testcases_created': 0,
            'configs_created': 0,
            'chunks_created': 0,
            'mysql_reason': '',
            'vector_reason': ''
        }
        
        page_id = page['id']
        title = page['title']
        
        # Check if already exists in MySQL
        if self.load_mysql and page_id in self._existing_checklists:
            result['mysql_reason'] = '–í–∂–µ —ñ—Å–Ω—É—î –≤ –ë–î'
        elif self.load_mysql:
            # Process for MySQL
            try:
                mysql_result = await self._process_page_mysql(page)
                if mysql_result['success']:
                    result['mysql_success'] = True
                    result['testcases_created'] = mysql_result['testcases_created']
                    result['configs_created'] = mysql_result['configs_created']
                    self._existing_checklists.add(page_id)
                else:
                    result['mysql_reason'] = mysql_result['reason']
            except Exception as e:
                result['mysql_reason'] = f'–ü–æ–º–∏–ª–∫–∞: {str(e)}'
        
        # Process for Vector DB
        if self.load_vector:
            try:
                vector_result = await self._process_page_vector(page)
                if vector_result['success']:
                    result['vector_success'] = True
                    result['chunks_created'] = vector_result['chunks_created']
                else:
                    result['vector_reason'] = vector_result['reason']
            except Exception as e:
                result['vector_reason'] = f'–ü–æ–º–∏–ª–∫–∞: {str(e)}'
        
        return result
    
    async def _process_page_mysql(self, page: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–æ–±–ª—è—î —Å—Ç–æ—Ä—ñ–Ω–∫—É –¥–ª—è MySQL (—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ QA –¥–∞–Ω—ñ) –∑ –≥—ñ–±—Ä–∏–¥–Ω–∏–º –ø—ñ–¥—Ö–æ–¥–æ–º."""
        try:
            page_id = page['id']
            title = page['title']
            
            # Get page content
            page_content = self.confluence_api.get_page_content(page_id)
            if not page_content:
                return {'success': False, 'reason': '–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç'}
            
            content = page_content.get('content', '')
            normalized_content = self.confluence_api.normalize_content(content)
            
            # –ì—ñ–±—Ä–∏–¥–Ω–∏–π –ø—ñ–¥—Ö—ñ–¥: HTML –ø–∞—Ä—Å–µ—Ä + AI –∞–Ω–∞–ª—ñ–∑
            html_testcases = []
            ai_testcases = []
            ai_configs = []
            
            # 1. –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ HTML –ø–∞—Ä—Å–µ—Ä
            html_start_time = datetime.now()
            try:
                html_testcases = self.html_parser.parse_testcases_from_html(content)
                html_duration = (datetime.now() - html_start_time).total_seconds()
                click.echo(f"  üîç HTML –ø–∞—Ä—Å–µ—Ä –∑–Ω–∞–π—à–æ–≤ {len(html_testcases)} —Ç–µ—Å—Ç–∫–µ–π—Å—ñ–≤ –∑–∞ {html_duration:.2f}—Å")
                
                # –õ–æ–≥—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É HTML –ø–∞—Ä—Å–µ—Ä–∞
                self._log_parser_stats("HTML", len(html_testcases), html_duration, True)
            except Exception as e:
                html_duration = (datetime.now() - html_start_time).total_seconds()
                click.echo(f"  ‚ö†Ô∏è HTML –ø–∞—Ä—Å–µ—Ä –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤: {e}")
                self._log_parser_stats("HTML", 0, html_duration, False, str(e))
            
            # 2. AI –∞–Ω–∞–ª—ñ–∑ —è–∫ –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è –∞–±–æ backup
            ai_start_time = datetime.now()
            try:
                if self.use_enhanced_analysis:
                    analysis_result = self.qa_analyzer.analyze_qa_content_enhanced(title, normalized_content)
                else:
                    analysis_result = self.qa_analyzer.analyze_qa_content(title, normalized_content)
                
                ai_testcases = analysis_result.testcases
                ai_configs = analysis_result.configs
                ai_duration = (datetime.now() - ai_start_time).total_seconds()
                click.echo(f"  ü§ñ AI –∞–Ω–∞–ª—ñ–∑ –∑–Ω–∞–π—à–æ–≤ {len(ai_testcases)} —Ç–µ—Å—Ç–∫–µ–π—Å—ñ–≤, {len(ai_configs)} –∫–æ–Ω—Ñ—ñ–≥—ñ–≤ –∑–∞ {ai_duration:.2f}—Å")
                
                # –õ–æ–≥—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É AI –∞–Ω–∞–ª—ñ–∑—É
                self._log_parser_stats("AI", len(ai_testcases), ai_duration, True, confidence=analysis_result.analysis_confidence)
            except Exception as e:
                ai_duration = (datetime.now() - ai_start_time).total_seconds()
                click.echo(f"  ‚ö†Ô∏è AI –∞–Ω–∞–ª—ñ–∑ –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤: {e}")
                self._log_parser_stats("AI", 0, ai_duration, False, str(e))
            
            # 3. –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            merge_start_time = datetime.now()
            all_testcases = []
            
            if len(html_testcases) > 10:  # –Ø–∫—â–æ HTML –∑–Ω–∞–π—à–æ–≤ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ
                click.echo(f"  üéØ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ HTML —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —è–∫ –æ—Å–Ω–æ–≤–Ω—ñ")
                all_testcases.extend(html_testcases)
                # –î–æ–¥–∞—î–º–æ AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —è–∫ –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è
                all_testcases.extend(ai_testcases)
                primary_method = "HTML"
            else:
                click.echo(f"  ü§ñ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —è–∫ –æ—Å–Ω–æ–≤–Ω—ñ")
                all_testcases.extend(ai_testcases)
                # –î–æ–¥–∞—î–º–æ HTML —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —è–∫ –¥–æ–ø–æ–≤–Ω–µ–Ω–Ω—è
                all_testcases.extend(html_testcases)
                primary_method = "AI"
            
            # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
            unique_testcases = self._remove_duplicates(all_testcases)
            merge_duration = (datetime.now() - merge_start_time).total_seconds()
            duplicates_removed = len(all_testcases) - len(unique_testcases)
            
            click.echo(f"  ‚úÖ –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ç–µ—Å—Ç–∫–µ–π—Å—ñ–≤ –ø—ñ—Å–ª—è –æ–±'—î–¥–Ω–∞–Ω–Ω—è: {len(unique_testcases)} (–≤–∏–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∑–∞ {merge_duration:.2f}—Å)")
            
            # –õ–æ–≥—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±'—î–¥–Ω–∞–Ω–Ω—è
            self._log_merge_stats(len(all_testcases), len(unique_testcases), duplicates_removed, primary_method, merge_duration)
            
            # Check if result is valid
            if not unique_testcases and not ai_configs:
                return {'success': False, 'reason': '–ù–µ–º–∞—î —Ç–µ—Å—Ç–∫–µ–π—Å—ñ–≤ –∞–±–æ –∫–æ–Ω—Ñ—ñ–≥—ñ–≤'}
            
            # Create checklist in DB
            session = self.qa_repo.get_session()
            try:
                # Find or create section (simplified - use first available section)
                section = session.query(QASection).first()
                if not section:
                    # Create default section
                    section = QASection(
                        confluence_page_id="default",
                        title="Default Section",
                        description="Default section for imported checklists",
                        url="https://confluence.togethernetworks.com/pages/default",
                        space_key=page_content.get('space', 'QMT')
                    )
                    session.add(section)
                    session.flush()
                
                # Create checklist
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                checklist = Checklist(
                    confluence_page_id=page_id,
                    title=title,
                    description=analysis_result.checklist_description or title,
                    additional_content=analysis_result.additional_content,
                    url=f"https://confluence.togethernetworks.com/pages/{page_id}",
                    space_key=page_content.get('space', 'QMT'),
                    section_id=section.id,
                    content_hash=content_hash,
                    version=page_content.get('version', 1)
                )
                
                session.add(checklist)
                session.flush()
                
                # Create testcases and configs
                configs_created = 0
                testcases_created = 0
                
                for testcase_data in unique_testcases:
                    # Create config if needed
                    config_id = None
                    if testcase_data.get('config_name'):
                        config = self._get_or_create_config(
                            session, testcase_data['config_name'], testcase_data.get('config_url')
                        )
                        if config:
                            config_id = config.id
                            configs_created += 1
                    
                    # Create testcase
                    if testcase_data.get('expected_result') and testcase_data.get('step'):
                        priority = testcase_data.get('priority')
                        if priority and priority not in ['LOWEST', 'LOW', 'MEDIUM', 'HIGH', 'HIGHEST', 'CRITICAL']:
                            priority = 'MEDIUM'
                        
                        # Handle screenshot
                        screenshot = testcase_data.get('screenshot')
                        if isinstance(screenshot, list) and screenshot:
                            screenshot = screenshot[0]
                        elif isinstance(screenshot, list):
                            screenshot = None
                        
                        testcase = TestCase(
                            checklist_id=checklist.id,
                            step=testcase_data.get('step', 'No step defined'),
                            expected_result=testcase_data.get('expected_result', 'No result defined'),
                            screenshot=screenshot,
                            priority=priority,
                            test_group=testcase_data.get('test_group'),
                            functionality=testcase_data.get('functionality'),
                            subcategory=testcase_data.get('subcategory'),
                            order_index=testcase_data.get('order_index', 0),
                            config_id=config_id,
                            qa_auto_coverage=testcase_data.get('qa_auto_coverage')
                        )
                        session.add(testcase)
                        testcases_created += 1
                
                session.commit()
                
                return {
                    'success': True,
                    'testcases_created': testcases_created,
                    'configs_created': configs_created
                }
                
            finally:
                session.close()
                
        except Exception as e:
            return {'success': False, 'reason': f'–ü–æ–º–∏–ª–∫–∞: {str(e)}'}
    
    async def _process_page_vector(self, page: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±—Ä–æ–±–ª—è—î —Å—Ç–æ—Ä—ñ–Ω–∫—É –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏."""
        try:
            # Normalize content
            normalized_content = self.confluence_api.normalize_content(page["content"])
            
            # Chunk the content
            chunks = self.chunker.chunk_text(normalized_content)
            
            if not chunks:
                return {'success': False, 'reason': '–ù–µ–º–∞—î –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è —á–∞–Ω–∫—ñ–≤'}
            
            # Generate embeddings for chunks
            chunk_embeddings = self.embedder.embed_batch([f"{page['title']}\n\n{chunk}" for chunk in chunks])
            
            # Upsert chunks to vector database
            chunks_data = []
            for i, (chunk_text, embedding) in enumerate(zip(chunks, chunk_embeddings)):
                if embedding is None:
                    continue
                
                chunk_id = f"{page['id']}_{i}"
                chunk_data = {
                    "chunk_id": chunk_id,
                    "embedding": embedding,
                    "document_id": page['id'],
                    "confluence_page_id": page["id"],
                    "title": page["title"],
                    "url": page["url"],
                    "space": page["space"],
                    "labels": page["labels"],
                    "feature_id": None,
                    "feature_name": None,
                    "chunk_ordinal": i,
                    "text": chunk_text
                }
                chunks_data.append(chunk_data)
            
            # Batch upsert to vector database
            successful_chunks, failed_chunks = self.vector_repo.upsert_chunks_batch(chunks_data)
            
            return {
                'success': True,
                'chunks_created': successful_chunks
            }
            
        except Exception as e:
            return {'success': False, 'reason': f'–ü–æ–º–∏–ª–∫–∞: {str(e)}'}
    
    def _remove_duplicates(self, testcases: List[Dict]) -> List[Dict]:
        """–ü–æ–∫—Ä–∞—â–µ–Ω–µ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∑ —Ç–µ—Å—Ç–∫–µ–π—Å—ñ–≤"""
        
        unique_testcases = []
        seen_hashes = set()
        
        for testcase in testcases:
            step = (testcase.get('step') or '').strip()
            expected = (testcase.get('expected_result') or '').strip()
            
            if not step or len(step) < 10:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫—ñ –∫—Ä–æ–∫–∏
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ —Ö–µ—à –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫—Ä–æ–∫—É —Ç–∞ –æ—á—ñ–∫—É–≤–∞–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
            testcase_hash = self._create_testcase_hash(step, expected)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç
            if testcase_hash not in seen_hashes:
                seen_hashes.add(testcase_hash)
                unique_testcases.append(testcase)
            else:
                # –Ø–∫—â–æ —Ü–µ –¥—É–±–ª—ñ–∫–∞—Ç, –∞–ª–µ –∑ –∫—Ä–∞—â–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–º—ñ–Ω—é—î–º–æ
                existing_index = self._find_existing_testcase_index(unique_testcases, testcase_hash)
                if existing_index is not None:
                    if self._is_better_testcase(testcase, unique_testcases[existing_index]):
                        unique_testcases[existing_index] = testcase
        
        return unique_testcases
    
    def _normalize_step_for_comparison(self, step: str) -> str:
        """–ù–æ—Ä–º–∞–ª—ñ–∑—É—î –∫—Ä–æ–∫ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è"""
        
        import re
        
        # –ü—Ä–∏–≤–æ–¥–∏–º–æ –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É
        normalized = step.lower().strip()
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ä–æ–∑–¥—ñ–ª–æ–≤—ñ –∑–Ω–∞–∫–∏
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        return normalized
    
    def _create_testcase_hash(self, step: str, expected: str) -> str:
        """–°—Ç–≤–æ—Ä—é—î —Ö–µ—à –¥–ª—è —Ç–µ—Å—Ç–∫–µ–π—Å—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫—Ä–æ–∫—É —Ç–∞ –æ—á—ñ–∫—É–≤–∞–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É"""
        import hashlib
        
        # –ù–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Ç–µ–∫—Å—Ç
        normalized_step = self._normalize_step_for_comparison(step)
        normalized_expected = self._normalize_step_for_comparison(expected)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ö–µ—à
        combined_text = f"{normalized_step}|{normalized_expected}"
        return hashlib.md5(combined_text.encode()).hexdigest()
    
    def _find_existing_testcase_index(self, testcases: List[Dict], testcase_hash: str) -> Optional[int]:
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å —ñ–Ω–¥–µ–∫—Å —ñ—Å–Ω—É—é—á–æ–≥–æ —Ç–µ—Å—Ç–∫–µ–π—Å—É –∑–∞ —Ö–µ—à–µ–º"""
        for i, testcase in enumerate(testcases):
            step = (testcase.get('step') or '').strip()
            expected = (testcase.get('expected_result') or '').strip()
            existing_hash = self._create_testcase_hash(step, expected)
            if existing_hash == testcase_hash:
                return i
        return None
    
    def _is_better_testcase(self, new_testcase: Dict, existing_testcase: Dict) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —î –Ω–æ–≤–∏–π —Ç–µ—Å—Ç–∫–µ–π—Å –∫—Ä–∞—â–∏–º –∑–∞ —ñ—Å–Ω—É—é—á–∏–π"""
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—É
        new_priority = new_testcase.get('priority')
        existing_priority = existing_testcase.get('priority')
        
        if new_priority and not existing_priority:
            return True
        if not new_priority and existing_priority:
            return False
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∫–æ–Ω—Ñ—ñ–≥—É
        new_config = new_testcase.get('config')
        existing_config = existing_testcase.get('config')
        
        if new_config and not existing_config:
            return True
        if not new_config and existing_config:
            return False
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ–≤–∂–∏–Ω—É –∫—Ä–æ–∫—É (–¥–æ–≤—à–∏–π = –∫—Ä–∞—â–∏–π)
        new_step_len = len(new_testcase.get('step', ''))
        existing_step_len = len(existing_testcase.get('step', ''))
        
        return new_step_len > existing_step_len
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–û–±—á–∏—Å–ª—é—î —Å—Ö–æ–∂—ñ—Å—Ç—å –º—ñ–∂ –¥–≤–æ–º–∞ —Ç–µ–∫—Å—Ç–∞–º–∏"""
        
        if not text1 or not text2:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Å–ø—ñ–ª—å–Ω–∏—Ö —Å–ª—ñ–≤
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _log_parser_stats(self, parser_type: str, testcases_count: int, duration: float, success: bool, error: str = None, confidence: float = None):
        """–õ–æ–≥—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–µ—Ä–∞"""
        import logging
        
        logger = logging.getLogger(__name__)
        
        log_data = {
            'parser_type': parser_type,
            'testcases_count': testcases_count,
            'duration_seconds': duration,
            'success': success,
            'timestamp': datetime.now().isoformat()
        }
        
        if error:
            log_data['error'] = error
        
        if confidence is not None:
            log_data['confidence'] = confidence
        
        if success:
            logger.info(f"Parser {parser_type}: {testcases_count} testcases in {duration:.2f}s", extra=log_data)
        else:
            logger.warning(f"Parser {parser_type} failed: {error}", extra=log_data)
    
    def _log_merge_stats(self, total_testcases: int, unique_testcases: int, duplicates_removed: int, primary_method: str, duration: float):
        """–õ–æ–≥—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±'—î–¥–Ω–∞–Ω–Ω—è"""
        import logging
        
        logger = logging.getLogger(__name__)
        
        log_data = {
            'total_testcases': total_testcases,
            'unique_testcases': unique_testcases,
            'duplicates_removed': duplicates_removed,
            'primary_method': primary_method,
            'duration_seconds': duration,
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"Merge completed: {unique_testcases} unique from {total_testcases} total (removed {duplicates_removed} duplicates) in {duration:.2f}s", extra=log_data)
    
    def _get_or_create_config(self, session, config_name: str, config_url: str = None) -> Optional[Config]:
        """–û—Ç—Ä–∏–º—É—î –∞–±–æ —Å—Ç–≤–æ—Ä—é—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é."""
        if not config_name:
            return None
        
        # Look for existing config
        config = session.query(Config).filter(Config.name == config_name).first()
        if config:
            return config
        
        # Create new
        config = Config(
            name=config_name,
            description=config_name,
            url=config_url
        )
        session.add(config)
        session.flush()
        return config
    
    def _create_ingestion_job(self, description: str) -> IngestionJob:
        """–°—Ç–≤–æ—Ä—é—î job –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è."""
        session = self.qa_repo.get_session()
        try:
            job = IngestionJob(details=description)
            session.add(job)
            session.commit()
            return job
        finally:
            session.close()
    
    def _update_ingestion_job(self, job: IngestionJob, status: str, details: Dict[str, Any]):
        """–û–Ω–æ–≤–ª—é—î job."""
        session = self.qa_repo.get_session()
        try:
            job.status = status
            job.details = f"Completed: {details}"
            job.documents_processed = details.get('checklists', 0)
            job.chunks_created = details.get('chunks', 0)
            job.features_created = details.get('configs', 0)
            session.merge(job)
            session.commit()
        finally:
            session.close()
    
    def _print_final_summary(self):
        """–í–∏–≤–æ–¥–∏—Ç—å —Ñ—ñ–Ω–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        click.echo(f"\nüéâ –û–ë'–Ñ–î–ù–ê–ù–ï –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û!")
        click.echo("=" * 60)
        click.echo(f"üìä –û–±—Ä–æ–±–ª–µ–Ω–æ —Å—Ç–æ—Ä—ñ–Ω–æ–∫: {self.progress.processed_pages}/{self.progress.total_pages}")
        if self.load_mysql:
            click.echo(f"üìã –°—Ç–≤–æ—Ä–µ–Ω–æ —á–µ–∫–ª—ñ—Å—Ç—ñ–≤: {self.progress.created_checklists}")
            click.echo(f"üß™ –°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–µ—Å—Ç–∫–µ–π—Å—ñ–≤: {self.progress.created_testcases}")
            click.echo(f"‚öôÔ∏è –°—Ç–≤–æ—Ä–µ–Ω–æ –∫–æ–Ω—Ñ—ñ–≥—ñ–≤: {self.progress.created_configs}")
            click.echo(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —á–µ–∫–ª—ñ—Å—Ç—ñ–≤: {self.progress.skipped_checklists}")
        if self.load_vector:
            click.echo(f"üîç –°—Ç–≤–æ—Ä–µ–Ω–æ —á–∞–Ω–∫—ñ–≤: {self.progress.chunks_created}")
    
    def close(self):
        """Clean up resources."""
        if self.load_mysql:
            self.qa_repo.close()
        if self.load_vector:
            self.vector_repo.close()


@click.command()
@click.option('--page-ids', help='Comma-separated list of root page IDs')
@click.option('--spaces', help='Comma-separated list of space keys (e.g., QA,ENG)')
@click.option('--labels', help='Comma-separated list of labels to filter by')
@click.option('--since', help='Load documents updated since this date (ISO format)')
@click.option('--limit', type=int, help='Maximum number of checklists to process')
@click.option('--use-config', is_flag=True, help='Use page IDs from config')
@click.option('--use-real-api', is_flag=True, help='Use real Confluence API instead of mock')
@click.option('--test-connection', is_flag=True, help='Test Confluence connection and exit')
@click.option('--mysql-only', is_flag=True, help='Load only to MySQL (skip vector DB)')
@click.option('--vector-only', is_flag=True, help='Load only to vector DB (skip MySQL)')
@click.option('--disable-enhanced-analysis', is_flag=True, help='Disable enhanced block-based analysis')
def main(page_ids, spaces, labels, since, limit, use_config, use_real_api, test_connection, mysql_only, vector_only, disable_enhanced_analysis):
    """Unified Confluence loader - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –≤ MySQL —Ç–∞ –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É."""
    
    # Validate environment
    if not settings.openai_api_key:
        click.echo("Error: OPENAI_API_KEY not set", err=True)
        sys.exit(1)
    
    # Determine what to load
    load_mysql = not vector_only
    load_vector = not mysql_only
    
    if not load_mysql and not load_vector:
        click.echo("Error: Must load to at least one destination (MySQL or Vector DB)", err=True)
        sys.exit(1)
    
    # Parse arguments
    space_keys = spaces.split(',') if spaces else None
    label_list = labels.split(',') if labels else None
    
    # Determine page IDs to load
    pages_to_load = None
    if use_config:
        if not settings.confluence_root_pages:
            click.echo("‚ùå CONFLUENCE_ROOT_PAGES –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ –≤ .env")
            sys.exit(1)
        pages_to_load = settings.confluence_root_pages.split(',')
    elif page_ids:
        pages_to_load = page_ids.split(',')
    
    # Auto-configure for real API if no specific parameters provided
    if use_real_api:
        if not pages_to_load and not space_keys:
            if settings.confluence_root_pages and settings.confluence_space_key:
                click.echo("Using configuration from .env file:")
                click.echo(f"  Space: {settings.confluence_space_key}")
                click.echo(f"  Root pages: {settings.confluence_root_pages}")
                pages_to_load = settings.confluence_root_pages.split(',')
            else:
                click.echo("‚ùå Error: CONFLUENCE_SPACE_KEY and CONFLUENCE_ROOT_PAGES must be set in .env file")
                return
        
        # Show current Confluence configuration
        click.echo(f"Confluence URL: {settings.confluence_base_url}")
        click.echo(f"Auth token: {'***set***' if settings.confluence_auth_token else 'NOT SET'}")
    
    # Initialize loader
    loader = UnifiedConfluenceLoader(
        use_mock=not use_real_api,
        load_mysql=load_mysql,
        load_vector=load_vector,
        use_enhanced_analysis=not disable_enhanced_analysis
    )
    
    # Test connection if requested
    if test_connection:
        if not use_real_api:
            click.echo("Connection test is only available with --use-real-api flag")
            sys.exit(1)
        
        click.echo("Testing Confluence connection...")
        result = loader.confluence_api.test_connection()
        
        if result["success"]:
            click.echo(f"‚úì Connection successful!")
            click.echo(f"  User: {result['user']}")
            click.echo(f"  Accessible spaces: {result['spaces_count']}")
            if result['spaces']:
                click.echo(f"  Spaces: {', '.join(result['spaces'])}")
            sys.exit(0)
        else:
            click.echo(f"‚úó Connection failed: {result['error']}")
            sys.exit(1)
    
    try:
        # Run the loading process
        result = asyncio.run(loader.load_data(
            page_ids=pages_to_load,
            space_keys=space_keys,
            labels=label_list,
            updated_since=since,
            limit=limit
        ))
        
        if result["success"]:
            sys.exit(0)
        else:
            sys.exit(1)
            
    except KeyboardInterrupt:
        click.echo("\n\nInterrupted by user", err=True)
        sys.exit(130)
    except Exception as e:
        click.echo(f"Unexpected error: {e}", err=True)
        sys.exit(1)
    finally:
        loader.close()


if __name__ == "__main__":
    main()
